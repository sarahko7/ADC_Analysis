---
title: "ADC_Code"
author: "Sarah Ko"
date: "April 7, 2019"
output: pdf_document
geometry: margin=2.54cm
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Set up your system

```{r}

# load packages
library(dplyr)
library(tidyverse)
library(ggplot2)
library(FSA)
library(lme4)
library(trend)

# set working directory
setwd("~/Duke/Year 2/Spring 2019/Data Analytics/ADC_Analysis/Code")

#check wd
getwd()

# create ggplot theme
SKotheme <- theme_gray(base_size = 15) +
  theme(axis.text = element_text(color = "black"), 
        legend.position = "right", 
        plot.title = element_text(hjust = 0.5))

# set ggplot theme
theme_set(SKotheme)

```

## Import & Explore

```{r}

# import dataset
ADC_raw <- read.csv("../Raw_Data/CalRecycle_ADC_raw.csv")

# explore dataset
view(ADC_raw)
class(ADC_raw)
colnames(ADC_raw)
dim(ADC_raw)

# per the CalRecycle website, segregation into ADC types started in 1998
# therefore, for the analysis, remove data from before 1998
class(ADC_raw$Report.Year)
ADC_data <- filter(ADC_raw, Report.Year >= 1998)
dim(ADC_data)

# explore new dataset
head(ADC_data)
tail(ADC_data)

# tidy the data by gathering the type columns
ADC_gathered <- gather(ADC_data, "Type", "Quantity", Ash:Sludge) %>%
  select(-Total) # remove Total column

# save the tidy dataset
write.csv(ADC_data, row.names = FALSE, file = "../Processed_Data/CalRecycle_ADC_tidy_processed.csv")

# generate summary data
ADC_summary_by_type <- ADC_gathered %>%
  group_by(Type) %>% # group the data by lakename
  filter(!is.na(Quantity)) %>% #remove the records when there are nas Quantity
  summarise(MeanQuarterlyQuantity = mean(Quantity), 
            MinQuarterlyQuantity = min(Quantity), 
            MaxQuarterlyQuantity = max(Quantity), 
            sdQuarterlyQuantity = sd(Quantity), 
            medianQuarterlyQuantity = median(Quantity))

ADC_summary_by_year <- ADC_gathered %>%
  group_by(Report.Year) %>% # group the data by year
  filter(!is.na(Quantity)) %>% #remove the records when there are nas Quantity
  summarise(MeanQuarterlyQuantity = mean(Quantity), 
            MinQuarterlyQuantity = min(Quantity), 
            MaxQuarterlyQuantity = max(Quantity), 
            sdQuarterlyQuantity = sd(Quantity), 
            medianQuarterlyQuantity = median(Quantity))

```

## Create Graphs

```{r}

# Graph 1: for 2017 data, display total by type
total_bytype_2017 <- ADC_gathered %>%
  filter(Report.Year == 2017) %>%
  group_by(Type) %>%
  summarize(Quantity = sum(Quantity))

# save 2017 dataset
write.csv(total_bytype_2017, row.names = FALSE, file = "../Processed_Data/CalRecycle_ADC_2017only_processed.csv")

# convert column Type into factor
class(total_bytype_2017$Type)
total_bytype_2017$Type <- as.factor(total_bytype_2017$Type)

# plot as a bar chart
total_bytype_2017_plot <- 
    ggplot(data=total_bytype_2017, aes(x=Type, y=Quantity)) +
    geom_bar(stat="identity") + 
    xlab('') + 
    ylab("Quantity (U.S. Tons)") +
    ggtitle("2017 Quantities of ADC by Type") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
    scale_x_discrete(labels = c('Ash','Auto Shredder Waste','Compost', 'Construction & Demolition Waste', 'Contaminated Sediment', 'Green Material', 'Mixed', 'Other', 'Sludge', 'Tires'))
print(total_bytype_2017_plot)

# save figure
ggsave("2017ADCbytype_alltypes.jpg", total_bytype_2017_plot, path = "../Output", height = 4, width = 6, units = "in", dpi = 300)

# Graph 2: faceted by Type, display spread of quarterly values by year
quarterlyvalues_byyear_plot <- ggplot(ADC_gathered) +
  geom_boxplot(aes(x = Report.Year, y = Quantity, group = Report.Year)) + 
  facet_wrap(vars(Type), nrow = 5) + 
  xlab("") +
  ylab("Quarterly Quantity (U.S. Tons)") +
  ggtitle("Quarterly Quantities of ADC, Grouped by Year")
print(quarterlyvalues_byyear_plot)

# save figure
ggsave("ADCyeardistribution_alltypes.jpg", quarterlyvalues_byyear_plot, path = "../Output", height = 8, width = 8, units = "in", dpi = 300)

# Graph 3: display data by quarter, all Types on same plot
quarterlyvalues_alltypes_plot <- 
  ggplot(ADC_gathered) + 
  geom_jitter(aes(x = Report.Quarter, y = Quantity, shape = as.factor(Type), color = as.factor(Report.Year)), width = 0.3, height = 0) + 
  labs(shape="Type", colour="Year") + 
  xlab("Quarter") + 
  ylab("Quantity (U.S. Tons)") + 
  ggtitle("Quantities Within Quarters") + 
  scale_shape_manual(values=c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), labels = c("Ash", "Auto Shredder Waste", "Compost", "Construction & Demolition", "Contaminated Sediment", "Green Material", "Mixed", "Other", "Sludge", "Tires")) + 
  theme(legend.position="right", legend.box = "vertical", legend.direction = "vertical") + 
  guides(shape = guide_legend(order = 1), color = guide_legend(order = 2))
print(quarterlyvalues_alltypes_plot)

# save figure
ggsave("QuarterlyADC_alltypes.jpg", quarterlyvalues_alltypes_plot, path = "../Output", height = 9, width = 9, units = "in", dpi = 300)

```

## Test 1: Statistical Modeling & Data Visualization

Is there a significant difference in total ADC between report quarters? (e.g. 1, 2, 3, 4)

```{r}

# create dataset with only total values, from 1995-2017
ADC_total_only <- ADC_raw %>%
  select(Report.Year, Report.Quarter, Total) # keep all columns except ADC Types

# convert column Report.Quarter into factor
class(ADC_total_only$Report.Quarter)
ADC_total_only$Report.Quarter <- as.factor(ADC_total_only$Report.Quarter) 

# save the dataset
write.csv(ADC_total_only, row.names = FALSE, file = "../Processed_Data/CalRecycle_ADC_totalsonly_processed.csv")

# perform one-way ANOVA
# assumption #0: observations are independent (cannot be tested, but assumed to be independent)

# test assumption #1: normality
# null hypothesis is that the dataset is normally distributed
shapiro.test(ADC_total_only$Total[ADC_total_only$Report.Quarter == 1]) # p-value = 0.03312
shapiro.test(ADC_total_only$Total[ADC_total_only$Report.Quarter == 2]) # p-value = 0.02271
shapiro.test(ADC_total_only$Total[ADC_total_only$Report.Quarter == 3]) # p-value = 0.00993
shapiro.test(ADC_total_only$Total[ADC_total_only$Report.Quarter == 4]) # p-value = 0.001305

ADC_freq_poly <- ggplot(ADC_total_only) +
  geom_freqpoly(aes(x = Total, color = Report.Quarter)) + 
  xlab("Quarterly Quantity (U.S. Tons)") + 
  ylab("# of Records")
print(ADC_freq_poly) # appears to be left skewed

qqnorm(ADC_total_only$Total); qqline(ADC_total_only$Total) # does not match 1:1 ratio

# Try to fix departure from normality with ln of Total. Result is not improved, so keep non-transformed data
ADC_LogTotal <- mutate(ADC_total_only, LogTotal = log(Total))
qqnorm(ADC_LogTotal$LogTotal); qqline(ADC_LogTotal$LogTotal)
bartlett.test(ADC_LogTotal$LogTotal ~ ADC_LogTotal$Report.Quarter)

# Try to fix departure from normality with 1/Total. Result is not improved, so keep non-transformed data
ADC_InvTotal <- mutate(ADC_total_only, InvTotal = 1/Total)
qqnorm(ADC_InvTotal$InvTotal); qqline(ADC_InvTotal$InvTotal)
bartlett.test(ADC_InvTotal$InvTotal ~ ADC_InvTotal$Report.Quarter)

# test assumption #2: equal variances among groups

# null hypothesis is that the variance is the same for the treatment groups
bartlett.test(ADC_total_only$Total ~ ADC_total_only$Report.Quarter) #p-value = 0.9308 # df = 3 (statistical power is very low)

# dataset is not normal, but does fulfill requirement for same variances. proceed with non-parametric tests.

# try non-parametric w/ post hoc, bc sample size is on the smaller end for parametric
ADC_quarter_kw <- kruskal.test(ADC_total_only$Total ~ ADC_total_only$Report.Quarter)
ADC_quarter_kw
dunnTest(ADC_total_only$Total, ADC_total_only$Report.Quarter)

# plot the results
ADC_quarter_plot <- ggplot(ADC_total_only, aes(x = Report.Quarter, y = Total)) +
  geom_violin(draw_quantiles = 0.5) + 
  xlab('Report Quarter') + 
  ylab('Quantity (U.S. Tons)') + 
  ggtitle('ADC Quantities by Quarter')
print(ADC_quarter_plot)

# save figure
ggsave("QuarterlyADC_violinplot.jpg", ADC_quarter_plot, path = "../Output", height = 4, width = 6, units = "in", dpi = 300)

```

## Test 2: Statistical Modeling & Data Visualization

Can total annual ADC be represented with a linear model?

```{r}

# assumptions for lm (independent observation, normal distribution, equal variances among groups) checked in Test 1. data is not normal, but group variances are equal. proceed with lm

# create dates corresponding to year & quarter combination
# Q1: Mar 31
# Q2: Jun 30
# Q3: Sep 30
# Q4: Dec 31

# create dataframe of month-date
quarters_to_dates <- data.frame("Quarter" = as.factor(1:4), "Month.Date" = c('3-31', '6-30', '9-30', '12-31'))

# create new dataframe with dates
ADC_fulldate <- ADC_total_only %>% 
  inner_join(quarters_to_dates, by = c("Report.Quarter" =  "Quarter")) %>%
  unite('Quarter.End.Date', c(Report.Year, Month.Date), sep = "-", remove = FALSE)

ADC_fulldate$Quarter.End.Date <- as.Date(ADC_fulldate$Quarter.End.Date, "%Y-%m-%d")
class(ADC_fulldate$Quarter.End.Date)

# create initial plot to visualize the data
ggplot(ADC_fulldate, aes(x = Quarter.End.Date, y = Total)) +
  geom_point()

# create lm
ADC_date_lm <- lm(data = ADC_fulldate, Total ~ Quarter.End.Date)
ADC_date_lm # Total = 73.14*Quarter.End.Date - 190264.58
summary(ADC_date_lm) # Adjusted R-squared:  0.4433 (date explains 44.33% of variation in total), p-value: 2.694e-13

# check normality of residuals
par(mfrow=c(2,2))
plot(ADC_date_lm) # QQ of residuals looks relatively normal

# plot data w/ model
ADC_fulldate_plot <- ggplot(ADC_fulldate, aes(x = Quarter.End.Date, y = Total)) +
  geom_abline(intercept = -190264.58, slope = 73.14) + 
  geom_point() + 
  xlab('') + 
  ylab('Quarterly Quantity (U.S. Tons)') + 
  ggtitle('Quarterly Quantities of ADC') 
print(ADC_fulldate_plot)
# visually, model does not appear to be a great fit

# save figure
ggsave("TotalADC_plot_calculatedmodel.jpg", ADC_fulldate_plot, path = "../Output", height = 4, width = 6, units = "in", dpi = 300)

# plot with loess smoother
ADC_fulldate_plot_loess <- ggplot(ADC_fulldate, aes(x = Quarter.End.Date, y = Total)) +
  geom_point() + 
  geom_smooth(method = loess) +
  xlab('') + 
  ylab('Quarterly Quantity (U.S. Tons)') + 
  ggtitle('Quarterly Quantities of ADC') 
print(ADC_fulldate_plot_loess)
# visually, model appears to be a great fit

# save figure
ggsave("TotalADC_plot_loess.jpg", ADC_fulldate_plot_loess, path = "../Output", height = 4, width = 6, units = "in", dpi = 300)

```

## Test 3: Statistical Modeling & Data Visualization

Is there a changepoint in the Construction & Demolition quantities over time?

```{r}

# create dataframe with dates
quarters_to_dates$Quarter <- as.integer(quarters_to_dates$Quarter)

CD_only <- ADC_data %>% 
  select(Report.Year, Report.Quarter, Construction.and.Demolition.Waste) %>%
  inner_join(quarters_to_dates, by = c("Report.Quarter" =  "Quarter")) %>%
  unite('Quarter.End.Date', c(Report.Year, Month.Date), sep = "-") %>%
  select(-Report.Quarter)

CD_only$Quarter.End.Date <- as.Date(CD_only$Quarter.End.Date, '%Y-%m-%d') # format column as date

# arrange data from oldest to newest
CD_only <- CD_only %>% 
  arrange(Quarter.End.Date)

# create initial plot to visualize the data
ggplot(CD_only, aes(x = Quarter.End.Date, y = Construction.and.Demolition.Waste)) +
  geom_point()

# check normality for C&D waste specifically
shapiro.test(CD_only$Construction.and.Demolition.Waste) # p-value = 0.4028, inferring that the data is normal

ggplot(CD_only) +
  geom_histogram(aes(x = Construction.and.Demolition.Waste))

qqnorm(CD_only$Construction.and.Demolition.Waste); qqline(CD_only$Construction.and.Demolition.Waste) # matches 1:1 ratio pretty well

# use Pettitt's test (nonparametric) to determine whether there is a shift in the central tendency of the time series. 
pettitt.test(CD_only$Construction.and.Demolition.Waste) # change point at time 40

# Run separate Mann-Kendall for each section
mk.test(CD_only$Construction.and.Demolition.Waste[1:40])
mk.test(CD_only$Construction.and.Demolition.Waste[41:80])

# Is there a second change point?
pettitt.test(CD_only$Construction.and.Demolition.Waste[41:80])
# position 27, so 41+27 = change point at time 68

# Run separate Mann-Kendall for new section
mk.test(CD_only$Construction.and.Demolition.Waste[69:80]) # p-value = 0.9453, not likely a 3rd change point

# Is there a third change point?
pettitt.test(CD_only$Construction.and.Demolition.Waste[69:80]) # p-value = p-value = 1.261, no 3rd change point

# years corresponding to changepoints
changepoint1 <- CD_only$Quarter.End.Date[40] # between Q4 2007 & Q1 2008 = ~ 2008-02-14
changepoint2 <- CD_only$Quarter.End.Date[68] # between Q4 2014 & Q1 2015 = ~ 2015-02-14

# Add vertical lines to the original graph to represent change points
CD_plot_changepoints <- ggplot(CD_only, aes(x=Quarter.End.Date, y=Construction.and.Demolition.Waste)) +
  geom_point() +
  geom_vline(aes(xintercept=as.Date('2008-02-14')), linetype=2, colour="purple", size=1) +
  geom_vline(aes(xintercept=as.Date('2015-02-14')), linetype=4, colour="blue", size=1) + 
  geom_text(x=as.Date('2010-1-1'), y=260000, label=stringr::str_wrap('changepoint1: Q4,2007-Q1,2008', 15), colour="purple", size=5) +
  geom_text(x=as.Date('2017-1-1'), y=260000, label=stringr::str_wrap('changepoint2: Q4,2014-Q1,2015', 15), colour="blue", size=5) +
  xlab('') +
  ylab('Quarterly C&D Cover (U.S. Tons)') + 
  scale_y_continuous(labels = scales::comma) + 
  ggtitle('Construction & Demolition Landfill Cover in CA')
print(CD_plot_changepoints)  

# save figure
ggsave("CD_plot_changepoints.jpg", CD_plot_changepoints, path = "../Output", height = 4, width = 11, units = "in", dpi = 300)

```

Misc code for Test 3: 

# Run separate seasonal Mann-Kendall for each change point
CD_as_ts <- ts(CD_only$Quarter.End.Date, start =1998-03-31, end =2017-12-31, frequency = 4) # convert vector of CD quantities into class ts

smk.test(ts(CD_as_ts[1:39], start =1998-03-31, end =2007-09-30, frequency = 4)) # p-value = 3.573e-05 inferring that there is monotonic trend over time with reporting season
smk.test(ts(CD_as_ts[40:80], start =2007-12-31, end =2017-12-31, frequency = 4))
SKO: decided not to use bc the fractions were smaller & smaller as you check changepoints, making the sample size smaller, which is worse for Mann-Kendall

---

#SKO: sample size is 22 for each group. used parametric (instead of non-parametric bc https://blog.minitab.com/blog/adventures-in-statistics-2/choosing-between-a-nonparametric-test-and-a-parametric-test)

# Format as an aov
ADC_quarter_anova <- aov(ADC_total_only$Total ~ ADC_total_only$Report.Quarter)
ADC_quarter_anova
summary(ADC_quarter_anova)

# Run a post-hoc test for pairwise differences
TukeyHSD(ADC_quarter_anova) # none of the p values are < 0.05
plot(TukeyHSD(ADC_quarter_anova)) # all of the bars overlap
# none of the pairings have significantly different means

--- 

# try Mann-Kendall non-parametric test to detect monotonic trends (H0: there is no trend)
total_oldest_to_newest <- ADC_fulldate %>%
  select(Quarter.End.Date, Total) %>%
  arrange(Quarter.End.Date) # arrange data from oldest to newest

mk.test(total_oldest_to_newest$Total) # p-value = 2.326e-09 inferring that there is a monotonic trend over time

# run seasonal Mann-Kendall
total_as_ts <- ts(total_oldest_to_newest$Total, start =1995-03-31, end =2017-12-31, frequency = 4) # convert total vector into class ts
smk.test(total_as_ts) # p-value < 2.2e-16 inferring that there is monotonic trend over time with reporting season

--- 

##SKO: create figures separately, then grid arrange

#Ash
ADC_gathered_Ash <- ADC_gathered %>%
  filter(Type == 'Ash')
quarterlyvalues_byyear_plot_Ash <- ggplot(ADC_gathered_Ash) +
  geom_boxplot(aes(x = Report.Year, y = Quantity, group = Report.Year))
print(quarterlyvalues_byyear_plot_Ash)

---

# group colors by 5 yr chunks: 1998-2002 (magenta), 2003-2007 (turquoise), 2008-2012 (red), 2013-2017 (yellow)
quarterlyvalues_alltypes_plot2 <- 
  ggplot(ADC_gathered) + 
  geom_point(data = subset(ADC_gathered, Report.Year < 2008), aes(x = Report.Quarter, y = Quantity, shape = as.factor(Type), color = as.factor(Report.Year < 2008), group = Report.Year)) #+ 
  scale_color_manual(values = c('Report.Year < 2008' = 'magenta3'))
  
print(quarterlyvalues_alltypes_plot2)

https://stackoverflow.com/questions/44915362/custom-grouping-for-legend-in-ggplot


